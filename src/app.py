import gradio as gr
import logging
from pathlib import Path
from typing import List, Dict, Tuple
from ingester import extract_text_from_pdf
from llm_chain import LLMChain
from retriever import DocumentRetriever
import hashlib

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Diret√≥rio para armazenar PDFs
UPLOAD_DIR = Path("../data/uploads")
UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

# Vari√°veis globais
current_context = ""
current_document_id = ""
chat_history = []
llm_chain = None
retriever = None


def process_pdf(file) -> str:
    """
    Processa um arquivo PDF, extrai texto e gera embeddings.
    """
    global current_context, current_document_id, retriever
    
    try:
        if file is None:
            return "‚ùå Nenhum arquivo selecionado"
        
        # Inicializar retriever se necess√°rio
        if retriever is None:
            retriever = DocumentRetriever()
        
        # Gradio retorna um dicion√°rio com 'name' e 'size'
        if isinstance(file, dict):
            file_path = file.get('name')
            if not file_path:
                return "‚ùå Erro: arquivo inv√°lido"
        else:
            file_path = file
        
        # Extrair texto
        text = extract_text_from_pdf(str(file_path))
        
        if not text:
            return "‚ùå Erro: PDF n√£o cont√©m texto ou est√° corrompido"
        
        # Extrair nome do arquivo
        file_name = Path(file_path).name if isinstance(file_path, str) else "documento.pdf"
        
        # Gerar ID √∫nico do documento
        current_document_id = hashlib.md5(file_name.encode()).hexdigest()[:16]
        
        # Armazenar contexto completo
        current_context = text
        
        # Adicionar ao banco de embeddings
        result = retriever.add_document(
            document_id=current_document_id,
            text=text,
            source=file_name
        )
        
        # Informa√ß√µes do arquivo
        file_size = len(text)
        preview = text[:500] + "..." if len(text) > 500 else text
        
        info = f"""‚úÖ **Arquivo processado com sucesso!**

üìÑ **Nome**: {file_name}
üìä **Caracteres**: {file_size}
üîç **Chunks gerados**: {result['count']}

**Preview**:
```
{preview}
```

üí° Agora voc√™ pode fazer perguntas sobre o documento usando busca sem√¢ntica!"""
        
        logger.info(f"PDF processado: {file_name} ({result['count']} chunks)")
        return info
        
    except Exception as e:
        logger.error(f"Erro ao processar PDF: {str(e)}")
        return f"‚ùå Erro: {str(e)}"


def send_message(user_message: str, history: List[Dict]) -> Tuple[str, List[Dict], str]:
    """
    Processa mensagem do usu√°rio com busca sem√¢ntica em embeddings.
    Retorna hist√≥rico no formato Gradio v6+ e informa√ß√µes completas de rastreamento.
    """
    global llm_chain, retriever, current_document_id
    
    if not user_message.strip():
        return "", history, ""
    
    chunk_info = ""
    
    try:
        # Inicializar LLMChain se necess√°rio
        if llm_chain is None:
            llm_chain = LLMChain(stream=False)
        
        # Buscar contexto relevante se houver documento carregado
        context = ""
        if current_document_id and retriever:
            try:
                # Buscar chunks relevantes COM RERANKING
                results = retriever.search(user_message, top_k=5, rerank=True)
                
                if results:
                    # Formatar informa√ß√µes completas dos chunks
                    chunk_parts = ["üîç **CHUNKS RECUPERADOS - RASTREAMENTO COMPLETO**\n"]
                    chunk_parts.append("=" * 100)
                    
                    for i, result in enumerate(results, 1):
                        # Cabe√ßalho do chunk com informa√ß√µes essenciais
                        chunk_parts.append(f"\n### **[{i}] CHUNK #{result['chunk']}**")
                        chunk_parts.append(f"üìÑ **Arquivo:** `{result['source']}`")
                        chunk_parts.append(f"üìç **P√°gina:** {result['page']}")
                        chunk_parts.append(f"üéØ **Relev√¢ncia:** {result['relevance']:.1%}")
                        chunk_parts.append(f"üÜî **ID:** `{result['id']}`")
                        chunk_parts.append(f"üìä **Dist√¢ncia:** {result['distance']:.4f}")
                        
                        # Conte√∫do completo
                        chunk_parts.append(f"\n**CONTE√öDO COMPLETO:**")
                        chunk_parts.append("```")
                        chunk_parts.append(result['text'])
                        chunk_parts.append("```")
                        chunk_parts.append("-" * 100)
                    
                    chunk_info = "\n".join(chunk_parts)
                    
                    # Usar contexto formatado para LLM
                    context = retriever.get_context(user_message, top_k=5, min_relevance=0.0)
                    logger.info("üîç Contexto sem√¢ntico recuperado com reranking")
                else:
                    chunk_info = "‚ö†Ô∏è Nenhum chunk relevante encontrado para esta pergunta."
                    
            except Exception as e:
                logger.warning(f"Erro ao recuperar contexto: {str(e)}")
                chunk_info = f"‚ùå Erro ao recuperar chunks: {str(e)}"
        else:
            chunk_info = "üì≠ Nenhum PDF carregado. Fa√ßa upload de um PDF para usar busca sem√¢ntica."
        
        # Tentar gerar resposta com a LLM
        system_prompt = (
            "Voc√™ √© um assistente √∫til que responde perguntas sobre documentos. "
            "Use o contexto fornecido para responder com precis√£o. "
            "Se a informa√ß√£o n√£o estiver no contexto, diga claramente."
        )
        
        response = llm_chain.generate_response(
            user_message=user_message,
            system_prompt=system_prompt,
            context=context if context else current_context
        )
        
        logger.info(f"Resposta gerada com sucesso")
        
    except Exception as e:
        # Se houver erro na API, informar o usu√°rio
        error_msg = f"‚ùå Erro ao processar mensagem:\n{str(e)}"
        response = error_msg
        logger.error(f"Erro ao chamar LLM: {str(e)}")
        chunk_info = f"‚ùå Erro: {str(e)}"
    
    # Adicionar mensagem e resposta ao hist√≥rico no formato Gradio v6+
    history.append({"role": "user", "content": user_message})
    history.append({"role": "assistant", "content": response})
    
    return "", history, chunk_info


def clear_chat() -> List[Dict]:
    """Limpa hist√≥rico do chat e o contexto."""
    global current_context, current_document_id, llm_chain, retriever
    current_context = ""
    
    # Remover documento do banco de embeddings
    if current_document_id and retriever:
        try:
            retriever.delete_document(current_document_id)
        except Exception as e:
            logger.warning(f"Erro ao deletar documento: {str(e)}")
    
    current_document_id = ""
    
    if llm_chain is not None:
        llm_chain.clear_history()
    
    return []


# Criar interface Gradio
with gr.Blocks(title="EixoAI") as demo:
    gr.Markdown("# üìö EixoAI - Chat com PDFs")
    gr.Markdown("Interface com busca sem√¢ntica e rastreamento de chunks")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### üìÑ Upload PDF")
            pdf_file = gr.File(
                label="Selecione um PDF",
                file_types=[".pdf"]
            )
            upload_btn = gr.Button("Processar PDF", variant="primary")
            pdf_info = gr.Markdown("Aguardando arquivo...")
            
            gr.Markdown("---")
            gr.Markdown("### üßπ A√ß√µes")
            clear_btn = gr.Button("Limpar Tudo")
        
        with gr.Column(scale=2):
            gr.Markdown("### üí¨ Chat")
            chatbot = gr.Chatbot(label="Hist√≥rico", height=400)
            
            with gr.Row():
                msg_input = gr.Textbox(
                    label="Mensagem",
                    placeholder="Digite sua mensagem...",
                    scale=4
                )
                send_btn = gr.Button("Enviar", scale=1)
            
            gr.Markdown("---")
            gr.Markdown("### üîç Rastreamento de Chunks")
            chunk_tracker = gr.Markdown(
                "Nenhum chunk recuperado ainda",
                label="Chunks Usados"
            )
    
    # Event handlers
    upload_btn.click(
        fn=process_pdf,
        inputs=[pdf_file],
        outputs=[pdf_info]
    )
    
    send_btn.click(
        fn=send_message,
        inputs=[msg_input, chatbot],
        outputs=[msg_input, chatbot, chunk_tracker]
    )
    
    msg_input.submit(
        fn=send_message,
        inputs=[msg_input, chatbot],
        outputs=[msg_input, chatbot, chunk_tracker]
    )
    
    clear_btn.click(
        fn=clear_chat,
        outputs=[chatbot]
    )


if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)